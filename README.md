# KG_Construction



Here, we are concentrate on collection of research papers relate to software engineering.   


Table of Contents
=================


<!--   * [Datasets / Shared Tasks](#Datasets_Shared_Tasks) -->
  * [Review on Multi-modal Data Analytics](#Review_on_Multi-modal_Data_Analytics)
  * [Multi-modal Information Extraction from Text](#Multi-modal_Information_Extraction_from_Text)
  * [Multi-modal Representation Learning](#Multi-modal_Representation_Learning)
  * [Multi-modal KG Construction](#Multi-modal_KG_Construction)
  * [Tutorials](#Tutorials)


<!-- ## Datasets_Shared_Tasks -->
## Review_on_Multi-modal_Data_Analytics
1. Yang Wang. [Survey on Deep Multi-modal Data Analytics: Collaboration, Rivalry and Fusion](https://arxiv.org/pdf/2006.08159.pdf). Arxiv 2020. [[Paper]](https://arxiv.org/pdf/2006.08159.pdf) 


## Multi-modal_Information_Extraction_from_Text
1. Manling Li, Alireza Zareian, Ying Lin, Xiaoman Pan, Spencer Whitehead, Brian Chen, Bo Wu, Heng Ji, Shih-Fu Chang, Clare Voss, Daniel Napierski, Marjorie Freedman. [GAIA: A Fine-grained Multimedia Knowledge Extraction System](https://www.aclweb.org/anthology/2020.acl-demos.11.pdf). ACL 2020. [[Paper]](https://www.aclweb.org/anthology/2020.acl-demos.11.pdf) (Best Demo Paper)


2. Shih-Fu Chang, LP Morency, Alexander Hauptmann, Alberto Del Bimbo, Cathal Gurrin, Hayley Hung, Heng Ji, Alan Smeaton. [Panel: Challenges for Multimedia/Multimodal Research in the Next Decade](https://blender.cs.illinois.edu/paper/multimediapanel.pdf). ACMMM 2019. [[Paper]](https://blender.cs.illinois.edu/paper/multimediapanel.pdf)


## Multi-modal_Representation_Learning
1. Huapeng Xu, Guilin Qi, Jingjing Li, Meng Wang, Kang Xu, Huan Gao. [Fine-grained Image Classification by Visual-Semantic Embedding](https://www.ijcai.org/Proceedings/2018/0145.pdf). IJCAI 2018. [[Paper]](https://www.ijcai.org/Proceedings/2018/0145.pdf)


## Multi-modal_KG_Construction
1. Hongzhi Li, Joe Ellis, Heng Ji, Shih-Fu Chang. [Event Specific Multimodal Pattern Mining for Knowledge Base Construction](https://blender.cs.illinois.edu/paper/acmmm2016.pdf). CSME 2018. [[Paper]](https://blender.cs.illinois.edu/paper/acmmm2016.pdf)


2. Meng Wang, Guilin Qi, Haofen Wang, Qiushuo Zheng. [Richpedia: A Comprehensive Multi-modal Knowledge Graph](https://link.springer.com/content/pdf/10.1007%2F978-3-030-41407-8_9.pdf). JIST 2019. [[Paper]](https://link.springer.com/content/pdf/10.1007%2F978-3-030-41407-8_9.pdf)



## Tutorials
1. Multi-modal Information Extraction from Text, Semi-structured, and Tabular Data on the Web. [[ACL 2020]](./tutorials/Multi-modal_Information_Extraction_from_Text.pdf)


2. Manning、 Ostendorf、 Povey、 何晓冬、 周明共话多模态NLP的机遇和挑战（附视频）. [[2020 北京智源大会  圆桌论坛 AI新疆域：多模态自然语言处理前沿趋势]](https://mp.weixin.qq.com/s?__biz=MzU5ODg0MTAwMw==&mid=2247488568&idx=1&sn=d9351b098be46f7bb69d18c6f59ac8a1&chksm=febf57fcc9c8deea89561f083767bd19baa649b2fbade7ab44ec8796a486d08788ce651aec35&mpshare=1&scene=1&srcid=07103OOC4gkqoQKgzyYFtWGN&sharer_sharetime=1594386259878&sharer_shareid=6a8a89e40ac625725a7e138018e905a5&key=b208b7ed0c58a19a9cdc2102a56d53caafab1f92eadca0b95197fda2b01425f0d321d6a7e2fa2fec28d910492ff301dd02853658fa611b4d3a4ba5c65896190e09908aed394c61812ba0133d2ec5613b&ascene=1&uin=NjI1MjE3OTQy&devicetype=Windows+10+x64&version=62090529&lang=zh_CN&exportkey=Adrm2w%2Fw1A3b0l%2Fg6a0g8eI%3D&pass_ticket=AmiZkESIKgJonY79YuRUaupucWvcklXJKVlGtFfjWtQE2bzHF%2BMV47H%2BkilE%2Fq80)
